{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install google-play-scraper\n",
        "!pip install sastrawi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9Tvx0uKmpSX",
        "outputId": "ec1c94cb-f1d6-4203-e7ac-91ece0a6de85"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-play-scraper in /usr/local/lib/python3.11/dist-packages (1.2.7)\n",
            "Requirement already satisfied: sastrawi in /usr/local/lib/python3.11/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyBDVP-Emv46",
        "outputId": "0c4acaa9-3ba5-46b6-bfea-6b9fdc0c8766"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries used for scraping and data analysis\n",
        "from google_play_scraper import app, reviews, Sort, reviews_all\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datetime as dt\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "from wordcloud import WordCloud\n",
        "import csv\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "BGFv87Bam3BM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set options\n",
        "pd.options.mode.chained_assignment = None\n",
        "seed = 0\n",
        "np.random.seed(seed)"
      ],
      "metadata": {
        "id": "t3o3lYqpm5-C"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scrapreview = reviews_all(\n",
        "    'com.mobile.legends',\n",
        "    lang='id',\n",
        "    country='id',\n",
        "    sort=Sort.MOST_RELEVANT,\n",
        "    count=20000\n",
        ")"
      ],
      "metadata": {
        "id": "cJmHhPzRm87S"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open('Hasil_Scraping_data.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Review'])\n",
        "    for review in scrapreview:\n",
        "        writer.writerow([review['content']])"
      ],
      "metadata": {
        "id": "y9ZdkWwxnNQ0"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}